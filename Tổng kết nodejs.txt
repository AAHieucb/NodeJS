# Basic
-> Có sẵn cli: node -> http.STATUS_CODES / Date.now()
Để update tool nvm quản lý phiên bản: ta phải tải lại từ github về file setup.exe và chạy mới lại
Chạy ứng dụng license có thể dùng: MIT, ISC, GPL-3.0
Install chuẩn phiên bản: npm install package-name@version --save
Dùng import require cần tránh circular dependencies

-> Dùng node-express-boilerplate => tạo dự án đủ mọi thứ



# Dùng stream
Cơ chế: chia file thành từng chunks và xử lý luôn, còn dùng bth sẽ load tất cả vào bộ nhớ và xử lý 1 lúc => nên dùng stream mọi lúc, trừ khi có nhu cầu đọc toàn bộ file 1 lúc
Với các file lớn mấy GB, dùng stream xử lý giúp tối ưu bộ nhớ gấp trăm lần và tốc độ cũng nhanh hơn, k bị tràn buffer.
VD đọc 1 file và ghi vào 1 file khác, fs.readFile(..., (err,data) => { fs.writeFile(...) }) nên thay thế bằng: (fs.createReadStream(...)).pipe(fs.createWriteStream(...));

Xem mem của 1 process nodejs trong win: chạy server như bth -> vào Task Manager tab Details -> tìm process tên là "node". Mỗi request sẽ thấy memory tiêu thụ tăng lên để xử lý 

-> Mỗi stream trong nodejs đều là 1 event emitter cho phép bắt sự kiện. 4 sự kiện có sẵn là data(đọc), end(hết dữ liệu để đọc), error, finish(khi hoàn thành)
Các stream trong NodeJS đều là chuỗi or buffer. Chuyển sang chế độ object mode của stream giúp nó thao tác với các kiểu giá trị khác của JS k là dạng buffer hay chuỗi



# Built-in module trong nodejs
URL tổng hợp: https://www.w3schools.com/nodejs/ref_modules.asp

-> Dùng buffer: Chỉ dùng khi cần dùng binary data từ file hay mạng, không phải chuỗi văn bản bth
NodeJS dùng 1 buffer pool chứa các buffer k còn được sử dụng. Khi cần dùng buffer sẽ lấy từ pool ra, nếu k có mới tạo mới 1 biến buffer khác. VD 1 buffer ta sử dụng lâu dài xuyên suốt mà k muốn được quản lý bởi pool thì có thể dùng Buffer.allocUnsafeSlow(<size>); thì biến đó sẽ k ảnh hưởng bởi pool hay garbage collector gì hết

-> Dùng child_process: thư viện hỗ trợ thao tác đa luồng trong nodejs
Dùng process.send trong nodejs có thể gửi data từ process con sang process cha.
Dùng thư viện @rauschma/stringio bổ trợ cho việc dùng child_process

--> Dùng spawn: chạy 1 lệnh nào trên 1 tiến trình con và trả ra cho stdio
--> Dùng exec: chạy lệnh nào trên 1 tiến trình con và bắt sự kiện
--> Dùng execFile: chạy lệnh nào trên 1 tiến trình con và trả ra cho buffer
--> Dùng fork: gọi fork 1 phát là file đó được thực hiện luôn trên 1 tiến trình riêng

-> Dùng os: Chrome V8 của gg là 1 js engine dùng thư viện libuv viết bằng C++ giúp tương tác với phần cứng và thực hiện các task nặng.
Biến môi trường process.env.UV_THREADPOOL_SIZE thiết lập kích thước threadpool mà libuv sử dụng cho các tác dụng async I/O. Defautl là 4 và có thể chỉnh để tận dụng tối đa số CPU cores của server. Còn nếu máy cùi ít cores thì tăng lên cũng v thôi.
VD có thể tự động tận dụng tối đa CPU cho server thông qua việc set giá trị biến môi trường UV_THREADPOOL_SIZE, còn nếu máy chạy server cùi ít cores thì đừng hy vọng



# Các package backend NodeJS thường dùng
-> Dùng node-persist: thư viện giúp tạo 1 global store tạm, tắt là mất.

-> Dùng yargs: xử lý tham số từ dòng lệnh cmd. Chú ý người dùng cũng có thể gửi tới server command và server dùng lib này để xử lý data
Dùng minimist đi kèm để parse process.argv thoải mái

-> crypto, crypto-js: sinh randomkey, mã hóa => thay bằng node:crypto có sẵn
-> Dùng bcrypt: thư viện hash. Có thể tùy chọn hash bao nhiêu vòng nhưng sẽ rất nặng
MD5 k còn được sử dụng vì mỗi string giống nhau cho đầu ra giống nhau.
Còn bcrypt sinh hash khác nhau với string giống nhau, nhờ dùng salt khi hash. Đb là khi verify lại k cần cái salt nữa mà vẫn check được password ban đầu là đúng với đoạn hash

-> Dùng cheerio: thư viện giúp parse DOM, tạo web crawler

-> Dùng chalk: package giúp style text trên console
Dùng figlet: package in chữ khổng lồ trên console
Dùng marked + marked-terminal: vẽ ngôn ngữ markdown lên terminal, có thể dùng kết hợp với chalk để custom style => chả bh dùng
Dùng clear: xóa terminal
=> Thường chỉ dùng khi cần tạo thư viện

-> Dùng faker: thư viện tạo mock data cực nhanh
-> Dùng node-mocks-http: mock http api 
-> Dùng supertest: test nhanh http. Hoạt động với mọi test framework or test 1 mình k cùng framework nào cũng đươc
-> Dùng ab: thư viện apache benchmark test rất mạnh
-> Dùng colors: thư viện hàng đầu khi cần in màu console.log trong terminal cho dễ nhìn 

-> Dùng winston: thư viện logger mạnh nhất nodejs.
Trong hệ thống lớn quan trọng về tốc độ, việc ghi log tốn thời gian thì thường chỉ push message vào buffer, rồi chạy 1 thread riêng lấy ra ghi.

-> Dùng module-alias: import module relative url gọn

-> Dùng Joi: thư viện validate type của object, email, password, not null => ref tới "Projects / BlogWeb"



# Các package chuyên dùng server
-> helmet: Tự thêm các header bảo mật cần thiết vào các request
Khi dùng thì 1s số lượng request có thể xử lý bị giảm đi vài nghìn, đánh đổi security và performance

-> cookie-parser: tương tác cookies

-> Dùng localtunnel: như ngrok nhưng nếu dùng trong code thì nó tiện hơn nhiều

-> http-errors: module tạo ra error với các mã lỗi của website, dùng nó trong 1 middleware để pass nó tới error handler tiếp theo.

-> Dùng http-status: thư viện thao tác với các loại status http

-> Dùng validator: giúp santinize string để chống XSS. Vd giúp check có phải email hay không chẳng hạn, tập hợp các type check có sẵn
Dùng xss-filter

-> Dùng express-flash + express-session: thư viện tạo flash message cho web nodejs
Flash message là message lưu tạm trong session hiện tại, nó là 1 dòng thông báo tạm thời và có thể truy xuất ở request tiếp theo, sau đó sẽ bị xoá. 
Tức là lib này cung cho server lưu 1 message vào session để truy xuất ở 1 request tiếp theo rồi tự xoá. Với CSR phía client tự lo nên chắc chỉ dùng cho SSR.

-> Dùng method-override:
Form HTMl chỉ hỗ trợ GET và POST từ thời xưa, để duy trì tính tương thích thì điều này vẫn giữ nguyên. Package này giúp người dùng chỉ gửi POST và nó tự sẽ chuyển đổi sang PATCH PUT DELETE tuỳ ý để xử lý

-> Dùng nodemon: thay thế với node --watch => chỉ watch ở development
Dùng concurrently: khi cần chạy song song client và server trong 1 dự án cùng lúc chỉ bằng 1 lệnh

-> Dùng lib apollo-server và graphql để xây 1 graphQL server



# Thao tác với file
Nên dùng stream, buffer

-> Dùng fs: Thao tác với file => ***Điều đặc biệt là nó có thể watch sự thay đổi của 1 file, điều mà database k làm được
Dùng lib rimraf: thư viện hỗ trợ xóa file và thư mục tương thích mọi hđh => chỉ be vì fe kbh cho xoá file
Dùng lib mv: move file tới bất cứ đâu. Mạnh hơn fs vì fs bị giới hạn bởi quyền, bản chất là nó xoá file vị trí cũ và create new file ở vị trí mới

-> Upload file lên server
- Dùng formidable: Xử lý files upload từ client. Phức tạp, tốc độ cao, chiếm ít bộ nhớ, bị hạn chế tính năng
- Dùng express-formidable: thay thế cho formidable khi dùng trong express. Nó khác với multer là k lưu lại gì vào server cả, tức k thể lưu file lại.
- Dùng multer: Cung cấp đầy đủ options và tính năng, code đơn giản. 
Có thể kết hợp cloudinary => Dùng luôn multer-storage-cloudinary cho dễ

FE nên gửi ảnh qua FormData, tương đương với <form action="/action_page_binary.asp" method="post" enctype="multipart/form-data"> trong html thuần.
Bên cạnh đó ảnh ta cũng cần phải lọc request như thế nào k thể để ai cũng DDoS đăng ảnh lên server đươc



# Dùng ExpressJS
-> Dùng middleware: khi có request thì các middleware chung k xđ url ưu tiên chạy trước rồi mới đến các middleware riêng của url đó
1 middleware có thể truyền dữ liệu vào req cho middleware tiếp theo lấy. Các middleware có sẵn đều tự gọi next(), chỉ có middleware ta tự tạo mới phải gọi next thủ công
Route cho middleware có thể là regexp, params, string pattern thoải mái

express.urlencoded({extended: true}) sẽ làm cho dữ liệu trong url mà người dùng gửi đến được lưu lại trong body của req dưới dạng key-value json
express.json() sẽ giúp lấy được req.body để truy cập vào cặp key-value trong url
express.static(<>) nếu cần dùng các file static

--> Middleware express.Router giúp chia 1 route ra thành nhiều route con
--> Middleware là error handler: hàm gồm 4 tham số err, req, res, next. Bth chỉ có 3 tham số req, res, next
--> Dùng kiểu app.all("*", (req,res,next) => {<>}); là route sau cùng để xử lý lỗi khi người dùng gọi router k tồn tại

-> NodeJS có thể xây API server or SSR cung ra html thông qua view engine
--> Dùng pug: luôn dùng vì tính tái sử dụng cao
--> Dùng ejs: cú pháp củ chuối k dùng

-> Dùng npx express-generator --view=pug myapp => boilerplate dùng view engine là pug

-> Dùng express-rate-limit: tránh ddos
-> Dùng connect-timeout: sinh lỗi network nếu request lâu quá 5s



# Dùng jsonwebtoken và bảo mật
Tối thiểu server phải có bảo mật SSL, chống XSS, database Injection, rate limit chặn DDoS, connect timeout.

AccessToken sinh từ {userid,role,timestamp,expiration} + secret key. Check password hợp lệ thì sinh accesstoken gửi cho client.
Expire time nhét vào payload và được check ở server chứ k lưu ở 1 trường FE để người dùng đổi tùy tiện. Chú ý k lưu thông tin nhạy cảm vào payload vì ai cũng giải mã ra được (jwt.io), chỉ có phần secret key trong token là k thể giải.

-> Dùng passport + passport-jwt kết hợp: sẽ dùng jwt như 1 strategy làm đơn giản code hơn
Dùng passport-local nếu xác thực tk mk tự làm

-> FE lưu refreshtoken tự động login
- Cookie: httpOnly cookie flag làm browser k đọc được cookie bằng JS. Cài cookie flag SameSite=strict và secure=true(chỉ HTTPs) để giảm thiểu CSRF.
- Indexed DB (ít dùng): là 1 NoSQL DB, dùng khi cần lưu 1 lượng data lớn và global trong code. Lưu được file, blobs. High performance với indexed, hỗ trợ transactions. Thường 1 app chỉ tạo 1 db dù ta có thể tạo bnh db cũng đươc. Cũng phải tạo connection, tạo object, lưu và chờ như db bth.
=> localStorage thì phải chống XSS mọi nơi nên cookie httpOnly an toàn hơn, nhưng các framework FE giờ rất mạnh để chống XSS r nên k đáng kể.
=> SSR nên dùng cookie, CSR nên dùng localStorage. Chỉ dùng indexed db khi muốn lưu như localStorage nhưng cần memory > 5MB

-> JWT cũng dùng khi quá trình verify bằng 1 pp bên ngoài mất nhiều tg thì chỉ cho làm 1 lần rồi sau đó dùng jwt xác thực cho nhanh. 
VD: ký message trong web3 xong rồi dùng jwt để giao tiếp.

-> Dùng redis vô hiệu hoá token
Usecase hệ thống bị hack và yêu cầu admin vô hiệu hóa 1 refresh token. Ta lưu tất cả refreshtoken vào redis và check thêm 1 bước, nếu nó k tồn tại trong redis, tức bị xoá hoặc expires thì login fail. Để vô hiệu hoá chỉ cần xoá khỏi redis. Tương tự nếu muốn vô hiệu accessToken
- Lưu redis kiểu key là user id, value là refreshtoken. Điều này đồng nghĩa token mới được lấy thì token cũ tự k dùng được nữa. 
- Nếu quản lý nhiều token 1 người, VD mỗi thiết bị 1 token thì ta lưu token sai vào (user => [blacklist token]) mỗi lần check phải check thêm token hết hạn chưa rồi xóa thủ công khỏi blacklist, hoặc tạo làm sao để tự xoá khi token qua expires time. Tương tự với pp whitelist.
=> Đây cũng là tính năng cho phép user logout mọi thiết bị hay các thiết bị cụ thể khi phát hiện bất thường

-> Bài toán: ứng dụng cập nhật có tính năng mới, chỉ ai có accesstoken mới mới dùng được, mọi accesstoken cũ cần bị hủy, tức buộc người dùng phải đăng nhập lại.
C1: Kể từ lúc cập nhật, mọi request của người dùng lên server lần đầu đều báo lỗi token k hợp lệ, yêu cầu login lại và lưu vào blacklist. Bất cứ ai chưa có trong blacklist đều báo k hợp lệ => K ổn nếu người dùng đăng nhập trên nhiều thiết bị và có nhiều token thì cái này chỉ báo có 1 lần là sai, các thiết bị khác sẽ pass cái blacklist

C2: Thay đổi cấu trúc token. 
1 JWT token được cấu tạo bởi: [(base64UrlEncoded JSON) Header] [(base64UrlEncoded JSON) Payload] [(base64UrlEncoded String) Signature]
Lưu version vào payload. VD ban đầu là {id, email} thì thêm thành {id, email, version}. Mỗi request ngoài check expiretime, check thêm version phải trùng với version của hệ thống. Version hệ thống có thể lưu trong env or từng user database. Khi update thì tăng version lên, accesstoken mới tạo ra cũng tăng version lên là được
Ta có thể custom chỉ update với list user xác định. Vd lưu vào user database thì đổi version của user nào sẽ chỉ hủy accessToken của user đó => Cũng có thể thêm tùy ý trường type vào payload để chỉ định áp dụng với user có type là gì
=> Tối ưu hơn nữa: thay vì báo lỗi có thể trả lại accessToken mới cho người dùng luôn nếu accesstoken đúng như version sai, FE phải check và update accessToken với mọi request

-> Xây dựng hệ thống tự phát hiện token bị hacker cướp: mặc định chơi 1 user có thể nhiều token vì login từ nhiều thiết bị.
Mỗi khi có login từ 1 thiết bị mới, gửi mail thông báo cho user về ip và location của thiết bị đó, để họ tự xác thực.
Khi AT hết hạn, FE nên gửi cả RT và AT cũ tới server. Khi làm mới token, server cũng nên làm mới cả RT và AT gửi cho user chứ k chỉ AT. Khi 1 RT được dùng để lấy cặp token mới, server nên lưu cả RT và AT và blacklist trong thời hạn expires time của nó. Nếu 1 request mà dùng lại AT và RT này, khả năng cao là token đã bị hack, server sẽ vô hiệu hoá thêm cả cặp token mới vào blacklist và gửi mail thông báo cho user luôn. Vì hacker có thể là người trước đó login hoặc là người mới login.

-> 2 cách dùng JWT:
- RSA => ref tới dự án "Ecommerce"
- HMAC => Lộ secret ở db là mất hết nên ít dùng so với RSA, ref tới "Tinkerbell garden"
Cơ chế: User login với username password -> Server dùng 1 HMAC secret key mã hoá thông tin payload thành signature, gửi cho user token = `payload.signature` -> user gửi kèm token ở mỗi request sau -> server check payload + HMAC secret key tạo lại đúng signature thì ok.
HMAC secret key có thể sinh random ở mỗi phiên và lưu vào db thay vì dùng 1 key cố định để đảm bảo signature khác nhau mỗi lần sinh và k thể dùng lại signature cũ.
Để chống replica attack trong 1 phiên, ta buộc sinh random HMAC secret key ở mỗi phiên lưu DB, sau đó gửi cho client lưu ở cả ở FE. Mỗi request, user phải ký lại payload kèm nonce và timestamp thành signature gửi kèm, server check signature đúng và timestamp k quá lâu và nonce chưa từng dùng là ok (nonce lưu kèm secret key trong db). Nếu lộ secret, sẽ chỉ lộ trong 1 phiên làm việc nên mới dám gửi cho FE như v



# Session và cookie
Cookie có giới hạn bộ nhớ nhỏ, được lưu ở phía client lâu dài.
Session là 1 loại data trao đổi giữa client và server. Session thường implement lưu data trong session storage cho 1 phiên làm việc thôi.
Có thể dùng cookie lưu session data tùy ý implement. Chú ý session storage là khái niệm chỉ có ở FE, backend k có session storage mà chỉ có cookie.

-> Server k thể xác định được các requests HTTP có phải từ cùng 1 người hay không vì HTTP là 1 giao thức k trạng thái, cookie ra đời đã giải quyết vấn đề đó. VD Setup cookie middleware lưu data tự động truyền giữa client server trong phạm vi 1 session.
=> Session chỉ nên lưu data 1 lần vào web, VD login data mà lưu lâu dài thì dùng jwt lưu cookies đi. Do đó session ít dùng trong dự án lớn.

-> Lưu session data ở server
Có thể dùng express-session lưu cookie trong memory (restart server là mất) hoặc cookie-session để lưu session data trong cookie (bị lỗi memory leak). Để fix, ta k được lưu session trong cookie nữa mà phải lưu trong các database khác bằng cách thêm option store:
- VD redis:
store: new RedisStore(),
- VD connect-mongo hỗ trợ lưu session data:
store: new MongoStore ({ 
  url: 'mongodb: // localhost: 27017 / test-app', // URL MONGODB CỦA BẠN 
  ttl: 14 * 24 * 60 * 60, // thời gian hết hạn 1 session
  autoRemove: 'native' 
}), => Đăng nhập vào mongo atlas sẽ thấy session được lưu ở đó

-> Đồng bộ session trong hệ phân tán: client gửi request tới nhiều server khác nhau, server 1 lưu session data, server 2 lưu sẽ invalid yêu cầu login lại
- Lưu session data chỉ ở client thông qua cookies / localStorage.
- Lưu ở local server thì cấu hình nginx trung gian thuật toán chỉ gửi 1 user tới 1 server là được
- Lưu ở 1 database or redis chung để nhiều server cùng lấy được, thường dùng cho SSO, nhược điểm là sập phát toang luôn
- Lưu ở local 1 server, rồi gọi đồng bộ copy session data vào các server khác => éo ổn


-> Usecase: trong hệ thống phân tán nhiều server, các server thường đồng bộ session data của user thông qua 1 redis db chung thay vì mỗi máy lưu 1 session data riêng. Vừa tối ưu bộ nhớ và tốc độ, vừa giúp đồng bộ user k cần login lại nhiều lần. 



# Dùng cors
Same Origin Policy (SOP) tự có sẵn trên browser cản mọi truy vấn khác domain để tăng tính bảo mật cho website. Mặc định server k set gì thêm thì sẽ bị cản, gây khó khăn cho dev. Do đó setup Cross Origin Resource Sharing(CORS) ở server sẽ cho phép cụ thể url nào khác domain nhưng vẫn được phép gọi
VD CSR truy cập vào API của 1 server từ frontend thì khi hosting frontend lên 1 domain thì server chỉ cần grant access cho domain đó.

-> Bản chất cors:
Nếu call API 1 server từ 1 domain của 1 website khác, vd vào 1 website bất kỳ mở F12 console lên gọi method GET tới 1 website khác thì origin là thứ lưu đường link của website client, đó là url trên thanh địa chỉ của website đó. Nếu ta call API từ ứng dụng hoặc đơn giản mở tên trang web trên browser bth thì origin ở đây là undefined

--> Các cấp độ để cấu hình cors:
- Nếu ta k cấu hình cors thì mặc định nó dùng SOP, ta k thể truy vấn từ mọi origin website khác nhưng lại có thể truy vấn từ origin undefined or vào trang web của nó để tự call đến chính nó. Còn vào 1 trang web lovehands mà call tới facebook thì k được.
Developer có thể disabled security mode trên trình duyệt or dùng các extension cho phép trình duyệt tự động có các header Allow-Access-Origin với các request là được.
- Nếu ta cấu hình cors cho phía server thì chỉ các trang web ta cho phép hoặc origin undefined mới truy cập đc vào API, cấm cả cùng domain cũng được. K cầm được origin undefined. VD: facebook dùng cách này đó, ta vẫn request nó được ở khi undefined, phải như v nó mới làm ra cái app messenger được chứ.
- Nếu ta dùng origin là 1 function như cách bên trên cors(<object có origin là 1 function>) thì ta có thể chơi kiểu lọc origin là gì nên quản lý đc kể cả origin là undefined ta cx cấm đc tùy ý ta

-> Có 2 loại request tới cors: simple request là các request k trigger cors preflight; preflighted request là các request có trigger cors preflight. 
Preflighted request thì trước tiên sẽ gửi 1 request OPTIONS tới server cần gọi, server sẽ gửi response tương ứng lại với request này chứa các thông tin cơ bản sau đó mới gửi request thật sự ta cần, kiểu check trước ấy => k đi sâu

-> Để dùng cors:
- Tự thêm header vào là xong nếu chỉ cần mỗi access-control-allow-origin 
- Dùng lib cros

Access-Control-Allow-Origin là url nào được gọi vào server
Access-Control-Allow-Credentials để là true thì được phép dùng cookies trong các request cors



# Dùng biến môi trường
Có sẵn biến môi trường: NODE_ENV=production node app.js -> lấy ra với process.env.NODE_ENV
NodeJS v18 hỗ trợ sẵn file .env k cần lib

-> Dùng dotenv: Theo thống nhất khi lập trình nodejs thì file .env ko được commit lên host theo git, để trong .gitignore
Dùng dotenv-expand: giúp tạo biến môi trường mà dựa trên biến môi trường khác
Dùng cross-env: có thể dùng quyết định môi trường nào ngay trong command. Nó còn ghê gớm khi tự động lấy tên file chuẩn trong config



# Nén data truyền qua API
Tốc độ sẽ nhanh hơn nhưng tăng áp lực lên CPU => nên dùng mọi lúc trừ khi CPU server yếu

compression: lib nén giảm lượng dữ liệu cần truyền qua mạng với gzip.
shrink-ray-current: dùng thuật toán brotli mới của google còn giúp nén mạnh hơn nữa.

Nhiều nền tảng như Cloudflare cũng hỗ trợ nén data: Client <---10KB--- Cloudflare <---100KB--- Origin server



# Dùng nodemailer 
Tạo tính năng gửi mail => refer tới "Projects / BlogWeb"

Khi truy cập vào gmail để gửi thông tin đi thì gmail sẽ chặn, để nodemailer hoạt động được thì:
- Với tài khoản bth: ta phải bật quyền truy cập của ứng dụng kém an toàn của google account trong mục bảo mật
- Với tài khoản xác minh 2 lớp (vd qua số đt nx): bảo mật -> App Password -> Sign in password -> SelectApp -> Other -> gõ tên mật khẩu -> sinh ra 1 password mới thay thế cho password hiện có để ứng dụng có tên đó truy cập vào đc gmail của ta 
URL: https://myaccount.google.com/security



# Dùng socketio
Cơ chế Socket.io xây dựa vào Engine.io, đầu tiên chạy long-polling để kết nối, r dùng các phương thức giao tiếp khác như websocket, nhưng chiếm ít tài nguyên hơn
Bắt sự kiện qua lại giữa socket client và server, broadcast event, tạo kênh và phòng, tự động kết nối lại, phát hiện ngắn kết nối, tạo channel nén dữ liệu khi gửi

VD: socket.volatile.emit('event_name', data); => volatile thì socket sẽ không cần đảm bảo rằng tin nhắn đã được nhận, giúp giảm độ trễ, gửi là xong luôn. Nếu dùng nó thì đang gửi mà disconnect thì bỏ luôn, nếu k dùng thì message được lưu lại tạm thời và khi reconnect sẽ gửi lại

-> Polling: query liên tục xem có thay đổi không. Duration càng ngắn, càng tốn tài nguyên.
Long-polling khắc phục nhược điểm của polling với ý tưởng: client gửi 1 request và server giữ lại request đó và sẽ response khi có 1 sự kiện tương ứng diễn ra. Nếu request client bị timeout, khi hết timeout nhưng vẫn chưa có sự kiện mong đợi nào thì server buộc trả về 1 response mà chả có dữ liệu nào, vẫn tốt hơn short-polling nhiều rồi.
Để tránh việc client bị treo hay ảnh hưởng hiệu suất của tác vụ khác, khi sử dụng long polling, nên thực hiện long polling trên một thread riêng => ít dùng

-> URL: https://socket.io/docs/v4/tutorial/step-7 => Nó dùng serverOffset để đánh dấu sẽ gửi message thiếu bù lại khi bị disconnect rồi reconnect.
Code theo luồng: VD tưởng tượng tạo phòng, rồi mn vào thì có data rồi làm gì thì có data gì, ta sẽ dễ dàng biết cái nào phát sự kiện và làm gì. VD lúc connect thì phát sự kiện nhận về data 
=> Socket đắt nên ta thường tạo phòng như battle hand, thay vì connect luôn, nếu k vào phòng thì chưa cần kết nối socket. Nên cho options user enable chứ k luôn tạo socket mỗi khi vào web
=> Để đảm bảo socket hoạt động, ui k update data ngay mà chờ socket gửi data mới về mới update. Data từ socket sẽ chỉ lấy từ socket, k cần fetch API. VD proj battlehand

-> Phía client hay server đều có thể tách file riêng cho export ra cái socket để thao tác, cô lập module => ref tới project "Battlehand"
VD cần gửi data cho 1 list socket thoả mãn đk: tạo singleton lưu list socket export từ file riêng; trong hàm on cũng có thể lấy list socket với io.sockets.sockets;



# Dùng json-server
Free: https://jsonplaceholder.typicode.com/posts

-> json-server / lite-server: có thể dùng 1 file text bth là db, dùng tạm thời và k có tính ACID của DB bth.

--> json-server --watch <link đến file db.json> --port 4000 => file db thay đổi sẽ update realtime. 
VD query: GET /employees/1/?_sort=firstName&amp;_order=desc
GET /employees?_page=7&amp;_limit=20



# Debug và log lỗi trong nodejs
node --inspect index.js -> mở devtool của chrome lên sẽ thấy option nodejs (k thấy thì gõ chrome://inspect/#devices) -> click là thấy console của nodejs trong browser
Console của browser sẽ giúp nodejs nhìn rõ chi tiết mọi object. Có thể tương tác trực tiếp in ra biến trong browser khi api chạy dừng ở breakpoint.
File js chạy phát tắt ngay thì k thể mở được devtool. Có thể cho server lắng nghe ở 1 cổng lâu để dùng. 
Click đúp vào dòng nào trong tab Source để đặt breakpoint, khi call api sẽ dừng ngay tại đó, F10 đi tiếp. Đặt "debugger;" trong code thì devtool browser cũng hiểu là dừng ở đó

-> morgan: thư viện log lại lịch sử gỡ lỗi và các request của user

-> Package debug: giúp log ở môi trường debug. 
VD: const debug = require("debug")("myapp:server"); debug("This is message");
=> Tức dùng debug với namespace có tên là myapp:server, namespace có vai trò lọc và phân loại các dòng log debug
Để dùng:
Trong linux: "start": "DEBUG=app:* node app.js" or chạy terminal trực tiếp DEBUG=app:* node app.js
Trong window: cd vào thư mục -> set DEBUG=appmyapp:* => để cho debug trong môi trường nào rồi chạy bth. Làm như này sẽ set biến môi trường của window trong phạm vi thư mục đó



# Bắt global error
Chạy node index.js có server listen và ta gọi throw luôn thì ct sẽ tự kết thúc vì lỗi unexpected k được bắt.
=> Kiểm soát bằng việc bắt các event lỗi: unhandledRejection, uncaughtException, SIGTERM. 
VD: process.on("uncaughtException", unexpectedErrorHandler); Trong unexpectedErrorHandler thg có process.exit(0);
=> Ta có thể override để tắt các connection tới database, socket các thứ. Thực ra nếu tắt process, nó cũng tự close các connection kia rồi, và các công nghệ hiện đại đủ để tự xử lý khi 1 bên tự bị close, phía bên kia cũng tự hiểu điều đó mà ta k cần làm gì thêm.



# Tương tác telegram
-> Tạo bot tự động nhắn tin vào channel sau 1 khoảng thời gian, có thể dùng thay thế gửi mail.
telegraf => giúp tương tác với telegram
node-schedule => package giúp tạo cron job, có thể setup chạy hàm gì vào thời điểm nào.

Quy trình: @BotFather => chat với thằng này để tạo bot, lấy API key, có thể edit và làm mọi thứ với bot.
Tạo 1 channel và thêm bot vào làm admin => ấn vào hình cái bút ở cuối list chat, ấn vào setting của channel
@JsonDumpBot => chat với thằng này để lấy thông tin phòng chat
Ta chat vào channel và forward message tơi @JsonDumpBot để lấy thông tin phòng chat, lấy được id channel trong forward_from_chat
=> Dùng channelid + api key là tạo được 1 con bot có thể sendmessage vào channel đó.



# Tool test
-> Jest: viết test case bằng JS để test cho FE. Nó tìm file test nằm trong thư mục __test__ or file có đuôi là .test.js
-> MounteBank: mock cung API có sẵn dạng ngân hàng => thà dùng json-server

-> Apache benchmark (ab): gửi nhiều request đồng thời tới 1 server để test performance => cài phức tạp, thay thế bằng autocannon
-> Autocannon: autocannon -c <> -p <> -d <> <url> 
-c là số lượng connection đồng thời, mặc định là 10 user cùng lúc
-p là đóng gói, bnh gói requests thì mặc định là 1
-d là số giây nó phản hồi lại, mặc định là 10
VD: autocannon http://localhost:5000/ thì sẽ check 1 gói 10 user chạy cùng lúc trong 10 giây thì mỗi giây có bnh request xử lý được thành công
VD với server bth test sẽ thấy nó cho average 900 request trong 1s, tức 9k request trong 10s với máy 4 threads. Khi xử lý mã hóa tốc độ nó giảm 10 lần



# Master require, import, module.export và export, dynamic import
Dùng module.exports = { x: new Class() } + require ở nhiều file khác sẽ chỉ chạy qua file 1 lần rồi lưu object vào cache. Dù file có thay đổi động thì cũng k chạy lại. Cái export ra dùng như 1 singleton.
Khi dùng async để khởi tạo biến, logic chạy như bth. Hàm async chạy xong thì require dùng dược biến, chưa chạy xong thì k dùng được.

-> Code
const x = new Class(); 
module.exports = x; hoặc module exports = { x }; hoặc module.exports = { x: new Class() };
=> Export singleton, có constructor

Class { static method() {} }
module.exports = Class;
const Class = require("./Class"); Class.method();
=> Export singleton 

module.exports = () => new Class(); 
const createInstance = require("./Class"); const x = createInstance();
Hoặc:
module.exports = Class;
const Class = require("./Class"); const x = new Class();
=> Mỗi lần dùng là 1 lần tạo mới

--> Có khởi tạo async
class MyClass { 
  constructor() { this.initialize(); } 
  async initialize() { } 
  static getInstance() { 
    if(!MyClass.instance) MyClass.instance = new MyClass();  
    return MyClass.instance; 
  } 
}
const instance = Database.getInstance();
module.exports = instance;
const instance = require('./file');
=> Init async và lấy singleton ra dùng. Đảm bảo initialize async hoàn thành mới dùng biến là được chạy.

-> Tùy biến lỗi async. VD gọi hàm async, bên trong hàm gán giá trị cho 1 biến và k chờ nó, sau đó ta export biến ra sẽ lỗi vì khi export, nó sẽ copy state hiện tại của biến vào cache (khi chưa thực hiện xong await) và bị sai.
VD: let x = 1; async function runSleep() { await sleep(2000); x = 2; }
runSleep(); module.exports = { x };

=> Để fix, phải export 1 hàm trong file trả về biến đó thì hàm bị cache, còn biến vẫn chạy và k bị xoá do hàm vẫn gọi tới, dùng singleton. VD: 
let x = 1; async function runSleep() { await sleep(2000); x = 2; } function getX(){ return x; }
runSleep(); module.exports = { getX: getX };
Hoặc thêm: let client = {}; client.x = x; module.exports = client; => nó sẽ cache client copy ngay, vùng nhớ x vẫn có ref tới nên k bị xóa, async chạy xong là vùng nhớ có giá trị và dùng thôi

-> Default nodejs dùng commonjs (require và module.exports), còn module dùng cho 1 vài gói riêng của nodejs ES6 (import export).
VD tùy biến package ESM cho phép dùng import/export trong commonjs
Cũng có dynamic import dùng được ở cả commonjs và ES6, phải import thông qua promise



# Chuyển hết về async await
-> Hàm callback
X(a, callback); => khi hàm X thực hiện xong sẽ truyền kết quả vào hàm callback ở cuối để xử lý. 
Thường hàm từ thư viện sẽ có sẵn phiên bản khác là async await. Vd: const x = await X(a); callback(x);

Nếu không có phiên bản async await, buộc phải truyền callback vào bth. Nếu callback là async, nó sẽ đi tiếp luôn và thực hiện song song hàm callback. Có thể ép chờ:
const x = await new Promise((resolve, reject) => {
  X(a, async (err, result) => {
    if(err) reject(err);
    resolve(result);
  })
})
=> Bản chất: promise kbh kết thúc nếu như cái cục cuối cùng của nó k gọi resolve or reject. Vd ở đây ta xóa resolve hay reject đi thì hàm promise này sẽ kbh kết thúc. Ta dựa vào nó để ép hàm này dùng được await.

-> Promise: await X().then();
Chú ý promise khi call resolve hay reject, nó vẫn chạy tiếp phần code bên dưới, do đó ta nên return để tránh nó đi tiếp
=> Trong callback phải check if(err) tuỳ lib, promise vẫn phải .then().catch, còn async await phải có try catch. Do đó chuyển sang async await phải luôn nhớ try catch error

-> Event với on
Ta có thể biến thành Promise để dùng với async. Bằng cách cho resolve khi onsucess, reject với onerror:
new Promise((resolve, reject) => {
  const reader = new FileReader();
  reader.onload = () => {
    resolve(reader.result);
  };
  reader.onerror = (error) => {
    reject(error);
  };
  reader.readAsDataURL(file);
});



# Dùng PM2
Tool chạy nhiều instance của server. Khi máy có nhiều CPU cores, PM2 sẽ tự tối ưu hóa bằng cách chạy mỗi process trên 1 core để tối ưu. Khi đó, PM2 tự dùng round robin để phân bố request cho các instance khác nhau. Còn nếu máy chỉ có 1 cores, việc chạy nhiều instance sẽ k có ích gì vì dùng chung core đó.
Khi server crash, pm2 sẽ auto restart nó, nên khi deploy sẽ kbh bị sập vì lỗi lạ.
Tuỳ ý kiểm soát từng process, giám sát resource (monit), tự động reload khi đổi như nodemon (--watch), xem log.
Có thể dùng file ecosystem.config.js để thiết lập cấu hình cho pm2 stable 

Khi chạy, nó tạo ra 1 process master lắng nghe ở cổng mà ta xác định trong file server. 
Ta có thể tuỳ chỉnh. VD: NODE_PORT=3002 pm2 start -i 2 app.js => rồi trong file app.js lấy với process.env.NODE_PORT (thực chất là cung 1 biến env). Sau đó VD ta chạy 2 instance thì nó sinh ra 2 worker process để xử lý, thông qua 1 process master lắng nghe ở 1 cổng. 

VD: pm2 deploy production/staging setup => lần đầu chạy nó pull code từ git về và setup, các lần sau chỉ cần: pm2 deploy production/staging update

-> option --no-daemon: 
Daemon là 1 loại chương trình trên các hệ điều hành like-unix hoạt động ẩn trong background. Các tiến trình daemon thg kết thúc bằng d như inetd, httpd, sshd, ipd. Các tiến trình daemon không thể bị gián đoạn và chỉ hoạt động khi có đầu vào, tách ra khỏi quá trình cha mẹ và thiết bị đầu cuối.
Nhiều tiến trình khi chạy phải thêm --no-daemon như câu lệnh pm2 start. VD khi dùng với docker thì nên có vì nếu chỉ chạy "pm2 start index.json" thì docker dừng luôn vì nó tưởng tiến trình đã kết thúc, chạy phát là dừng do nó ở trạng thái daemon và k có đầu vào nên k hoạt động.

-> Dùng PM2 có 2 chế độ là forkmode và cluster mode
- Fork mode: Chạy 1 instance duy nhất với "pm2 start app.js --name my-api"
Nếu muôn chạy nhiều instance thì chạy nhiều lần script đó. Mỗi instance là độc lập và hệ điều hành kiểm soát việc phân bổ vào CPU core nào, có thể là cùng cores.
- Cluster mode: VD "pm2 start app.js -i 0" sẽ chạy ứng dụng với max số CPU cores
Cluster mode sẽ tự phân bổ instance vào các cores khác nhau. PM2 cũng tự dùng cơ chế load balancing là round-robin



# Tối ưu với cluster
Cluster là 1 nhóm máy tính (nodes) làm việc cùng nhau để chia tải công việc, cùng cung ra 1 dịch vụ. VD 1 hệ thống có 1 chức năng cụ thể, ta mở rộng bằng cách triển khai chức năng đó trên nhiều máy khác nhau mà hoạt động như 1 máy duy nhất cung ra dịch vụ chức năng đó => chính là hệ phân tán
Tương tự việc chạy nhiều instance cùng 1 chức năng trên 1 máy để tận dụng tối đa số CPU cores trong máy. Vì luôn muốn đủ máy xử lý lượng tải lớn và mỗi máy đều dùng tối ưu CPU

Nếu cùng 1 máy, Các instance độc lập nhau về main memory, còn đĩa cứng với file thì vẫn chung
Chỉ đúng với nodejs là ngôn ngữ đơn luồng, còn ASP.NET thông qua thread async await tự chia task tối ưu cho multicore CPU rồi.

-> NodeJS cũng có sẵn package cluster chuyên chạy nhiều instance của server trên 1 máy: https://www.youtube.com/watch?v=_74_z2-uOys
Cơ chế khi chạy sẽ tạo 1 instance isMaster, fork ra hàng loạt instance isWorker chạy dự án lại từ trên xuống. Master có thể dùng quản lý các instance worker. 
Nó tự tận dụng mỗi core CPU là 1 process, mỗi process vẫn đơn luồng trong nodejs, sẽ chịu tải đồng thời tốt hơn. 
Số instance k nên set là max số CPU cores mà nên trải qua thực nghiệm để tối ưu. VD máy 10 cores chạy được 20 instances thì chỉ nên chạy tầm 10 intances, còn lại dành cho task khác để tránh quá tải sẽ chậm hơn trước. Ta chạy chung 1 cổng và nó sẽ tự dùng cơ chế load balancing tối ưu của hđh để xử lý, thường là round robin, ta k cần đụng

-> Dùng PM2 cũng tạo instance phân bố vào cores khác nhau tối ưu => nếu chả mất gì mà lại tăng performance thì tội gì k dùng.


